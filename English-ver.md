## The Wisdom of the Crowd

### The Estimate of the weight of an ox
from **superforecasting**
> In 1906 the legendary British scientist Sir Francis Galton went to a country fair and watched as hundreds of people individually guessed the weight that a live ox would be after it was “slaughtered and dressed.” Their average guess—their collective judgment—was 1,197 pounds, one pound short of the correct answer, 1,198 pounds. It was the earliest demonstration of a phenomenon popularized by—and now named for—James Surowiecki’s bestseller **The Wisdom of Crowds**. Aggregating the judgment of many consistently beats the accuracy of the average member of the group, and is often as startlingly accurate as Galton’s weight-guessers. The collective judgment isn’t always more accurate than any individual guess, however. In fact, in any group there are likely to be individuals who beat the group. But those bull’s-eye guesses typically say more about the power of luck—chimps who throw a lot of darts will get occasional bull’s-eyes—than about the skill of the guesser. That becomes clear when the exercise is repeated many times. There will be individuals who beat the group in each repetition, but they will tend to be different individuals. Beating the average consistently requires rare skill.

### Guess-the-Number game
from **superforecasting**
> Consider a guess-the-number game in which players must guess a number between 0 and 100. The person whose guess comes closest to two-thirds of the average guess of all contestants wins. That’s it. And imagine there is a prize: the reader who comes closest to the correct answer wins a pair of business-class tickets for a flight between London and New York.

### What could 'Good Judegment Project' reveal?
from **superforecasting**

> Here's one possible revealtion: Imagine you get a couple of hundred ordinary people to forecast geopolitical events. You see how often they revise their forecasts and how accurate those forecasts prove to be and use that information to identify the forty or so who are the best. Then you have everyone make lots more forecasts. This time, you calculate the average forecast of the whole group--"the wisdom of the crowd"--but with extra weight given to those forty top forecasters. Then you give the forecast a final tweak: You "extremize" it, meaning you push it closer to 100% or zero. If the forecast is 70% you might bump it up to, say, 85%. If it's 30%, you might reduce it to 15%.

## fallacy

### Randomness
from **superforecasting**
> The psychologist Ellen Langer has shown how poorly we grasp randomness in a series of experiments. In one, she asked Yale students to watch someone flip a coin thirty times and predict whether it would come up heads or tails. The students could not see the actual flipping but they were told the results of each toss. The results, however, were rigged: all students got a total of fifteen right and fifteen wrong, but some students got a string of hits early while others started with a string of misses. Langer then asked the students how well they thought they would do if the experiment were repeated. Students who started off with a string of hits had a higher opinion of their skill and thought they would shine again. Langer called this the “illusion of control,” but it is also an “illusion of prediction.”

> Outside Yale labs, delusions of this sort are routine. Watch business news on television, where talking heads are often introduced with a reference to one of their dramatic forecasting successes: “Pedro Ziff called the crash of 2008!” The point is to make them credible so we’ll want to hear their next forecast. But even if we assume these statements are true accounts of what the person forecast—they often are not—they tell us next to nothing about the guest’s accuracy, as viewers would know if they applied a little System 2 thought. Even a dart-throwing chimp will hit the occasional bull’s-eye if he throws enough darts, and anyone can easily “predict” the next stock market crash by incessantly warning that the stock market is about to crash. And yet many people take these hollow claims seriously.

## Problem Solving

### How many piano tuners are there in Chicago?
from **superforecasting**
>Well, the number of piano tuners depends on how much piano-tuning work there is and how much work it takes to employ one piano tuner. So I could nail this question if I knew four facts: 
> 1. The number of pianos in Chicago 
2. How often pianos are tuned each year 
3. How long it takes to tune a piano 
4. How many hours a year the average piano tuner works

